from tavily import TavilyClient
import trafilatura
from sentence_transformers import SentenceTransformer
import numpy as np
from llama_cpp import Llama
from config import Settings

# -------------------------
# CONFIG
# -------------------------
MAX_DOC_CHARS = 1200        # per document
MAX_TOTAL_CHARS = 3000      # total context (SAFE for 4k ctx)
SIM_THRESHOLD = 0.10

settings = Settings()
tavily_client = TavilyClient(api_key=settings.TAVILY_API_KEY)


class WebFallbackService:
    def __init__(self, model_path: str):
        self.embedding_model = SentenceTransformer("all-miniLM-L6-v2")

        self.llm = Llama(
            model_path=model_path,
            n_ctx=4096,
            n_threads=8,
            n_gpu_layers=0
        )

    # -------------------------
    # Web search + scrape
    # -------------------------
    def web_search(self, query: str):
        print("\nЁЯФН [WEB SEARCH TRIGGERED]")
        print("ЁЯФО Query:", query)

        results = []
        response = tavily_client.search(query, max_results=5)

        for r in response.get("results", []):
            url = r.get("url")
            print("ЁЯМР URL:", url)

            downloaded = trafilatura.fetch_url(url)
            if not downloaded:
                print("тмЗя╕П Downloaded HTML: NO")
                continue

            content = trafilatura.extract(downloaded, include_comments=False)
            if not content or len(content) < 200:
                print("тЪая╕П Skipped: insufficient content")
                continue

            content = content[:MAX_DOC_CHARS]

            print("ЁЯУД Extracted length:", len(content))

            results.append({
                "title": r.get("title", ""),
                "url": url,
                "content": content
            })

        print("тЬЕ Total usable web documents:", len(results))
        return results

    # -------------------------
    # Sort by relevance
    # -------------------------
    def sort_sources(self, query: str, docs: list[dict]):
        print("\nЁЯза [SORTING SOURCES]")
        relevant = []

        q_emb = self.embedding_model.encode(query)

        for d in docs:
            emb = self.embedding_model.encode(d["content"])
            sim = np.dot(q_emb, emb) / (
                np.linalg.norm(q_emb) * np.linalg.norm(emb)
            )

            print(f"ЁЯУК Similarity {sim:.3f} тЖТ {d['url']}")

            if sim >= SIM_THRESHOLD:
                d["score"] = sim
                relevant.append(d)

        relevant.sort(key=lambda x: x["score"], reverse=True)
        print("тЬЕ Relevant documents kept:", len(relevant))
        return relevant

    # -------------------------
    # Final answer
    # -------------------------
    def answer(self, query: str) -> str:
        print("\nЁЯзк [WEB FALLBACK ANSWER PIPELINE]")

        search_results = self.web_search(query)
        ranked = self.sort_sources(query, search_results)

        if not ranked:
            return "роЗрогрпИропродрпНродро┐ро▓рпН родрпКроЯро░рпНрокрпБроЯрпИроп родроХро╡ро▓рпН роХро┐роЯрпИроХрпНроХро╡ро┐ро▓рпНро▓рпИ."

        # ЁЯФР SAFE CONTEXT BUILD
        context_parts = []
        total_chars = 0

        for i, r in enumerate(ranked):
            block = f"Source {i+1}:\n{r['content']}\n"
            if total_chars + len(block) > MAX_TOTAL_CHARS:
                break
            context_parts.append(block)
            total_chars += len(block)

        context = "\n".join(context_parts)

        print("ЁЯУЪ Total context size:", total_chars, "chars")

        prompt = f"""
роирпАроЩрпНроХро│рпН роТро░рпБ роЕро▒ро┐ро╡ро╛ро░рпНроирпНрод роЙродро╡ро┐ропро╛ро│ро░рпН.

роХрпАро┤рпЗ роЗрогрпИропродрпНродро┐ро▓рпН роЗро░рпБроирпНродрпБ рокрпЖро▒рокрпНрокроЯрпНроЯ родроХро╡ро▓рпНроХро│рпН роЙро│рпНро│рой.
роЗроирпНрод родроХро╡ро▓рпНроХро│рпИ ро╡рпИродрпНродрпБ роХрпЗро│рпНро╡ро┐роХрпНроХрпБ роирпЗро░роЯро┐ропро╛роХ рокродро┐ро▓ро│ро┐роХрпНроХро╡рпБроорпН.

тЭМ роЪрпБро░рпБроХрпНроХроорпН роЪрпЖропрпНроп ро╡рпЗрогрпНроЯро╛роорпН
тЭМ Instruction / Response роОройрпНро▒рпБ роОро┤рпБрод ро╡рпЗрогрпНроЯро╛роорпН
тЬЕ роЗро▒рпБродро┐ рокродро┐ро▓рпИ роороЯрпНроЯрпБроорпН ро╡ро┤роЩрпНроХро╡рпБроорпН

родроХро╡ро▓рпНроХро│рпН:
{context}

роХрпЗро│рпНро╡ро┐:
{query}

рокродро┐ро▓рпН:
"""

        output = self.llm(
            prompt,
            max_tokens=512,
            temperature=0.3,
            top_p=0.9,
            stop=["</s>"]
        )

        answer = output["choices"][0]["text"].strip()
        print("тЬЕ [WEB RAG] Answer generated")
        return answer
